#Life Sciences RAG Assistant

A Retrieval-Augmented Generation (RAG) pipeline designed for pharmaceutical document intelligence. This solution enables users to upload PDFs (e.g., drug monographs, clinical trial summaries, medical guidelines), build a searchable knowledge index, ask medical questions, and receive grounded, context-based answers.

â¸»

ğŸ” Key Capabilities
	â€¢	Upload pharma PDFs and build vector index
	â€¢	Query drug-related information
	â€¢	Generates context-aware and factual answers using LLM
	â€¢	On-demand summarization of PDFs
	â€¢	Embedded RAG evaluation script for precision scoring

â¸»

ğŸ“ Project Structure

rag_pipeline/
â”‚â”€â”€ app.py                 # Streamlit UI
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ build_index.py     # PDF loading, chunking, vector store creation
â”‚   â”œâ”€â”€ rag_pipeline.py    # Retriever + RAG chain creation
â”‚   â”œâ”€â”€ llm_config.py      # LLM + embedding config
â”‚   â”œâ”€â”€ summarize.py       # PDF summarization pipeline
â”‚   â””â”€â”€ evaluate_rag.py    # RAG performance evaluation
â””â”€â”€ vector_store/          # Persisted Chroma DB


â¸»

âš™ï¸ How It Works

1ï¸âƒ£ Upload PDF & Build Index
	â€¢	Loads PDF pages
	â€¢	Splits into 1000-char chunks (150 overlap)
	â€¢	Embeds using MiniLM HuggingFace embeddings
	â€¢	Saves vectors in ChromaDB

2ï¸âƒ£ Ask a Question
	â€¢	Query is embedded
	â€¢	Hybrid search retrieves most relevant text
	â€¢	Claude Opus answers only using retrieved context

3ï¸âƒ£ Optional â€” PDF Summary
	â€¢	Map-reduce summarization pipeline
	â€¢	Provides quick, structured medical insights

4ï¸âƒ£ Evaluate RAG
	â€¢	Cosine-similarity scoring vs human ground truths
	â€¢	Retrieval & generation accuracy metrics

â¸»

ğŸ§  Tech Stack & Justification

Component	Choice	Reason
Vector DB	Chroma	Lightweight, fast, persistent, local
Embeddings	MiniLM	Small, accurate, fast for pharma domain
LLM	Claude-3 Opus	Strong factual grounding & medical reasoning
Framework	LangChain	Modular RAG orchestration, retrievers, wrappers
UI	Streamlit	Rapid prototyping + user-friendly


â¸»

ğŸš€ Running the Application

Install dependencies

pip install -r requirements.txt

Run Streamlit app

streamlit run app.py

Build Vector Index

Click â€œBuild Indexâ€ in UI after uploading PDF

Ask Questions & View Answers

Type your query in the UI and hit Search

Evaluate RAG

python src/evaluate_rag.py


â¸»

ğŸ“Š Evaluation Method
	â€¢	10 curated pharma questions (Repatha/Aimovig)
	â€¢	Cosine similarity scoring
	â€¢	Reports retrieval vs generation performance

â¸»

ğŸ“Œ Business Value
	â€¢	Accelerates access to medical knowledge
	â€¢	Reduces manual search across 300-page drug docs
	â€¢	Improves medical call-center productivity
	â€¢	Enables medical reps & clinicians to get factual info fast
	â€¢	Customizable for pharmacovigilance, medical affairs, clinical teams

â¸»

ğŸ”® Future Enhancements
	â€¢	BM25 + dense hybrid retriever
	â€¢	Reranking via cross-encoder (bio-med models)
	â€¢	RAG hallucination guardrails
	â€¢	Multi-document ingestion & multi-file search



